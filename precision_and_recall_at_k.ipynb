{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T20:13:41.940705Z",
     "start_time": "2021-02-09T20:13:41.715611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '20G', 'executorMemory': '20G', 'executorCores': 16, 'numExecutors': 20, 'name': 'amir_weighted_trend_eval', 'conf': {'spark.dynamicAllocation.enabled': 'false', 'spark.pyspark.python': 'python3', 'spark.sql.broadcastTimeout': 10000}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>164</td><td>application_1600172976670_6157</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6157/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-86-133.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6157_01_000001/livy\">Link</a></td><td></td></tr><tr><td>167</td><td>application_1600172976670_6160</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6160/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-82-135.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6160_01_000001/livy\">Link</a></td><td></td></tr><tr><td>168</td><td>application_1600172976670_6161</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6161/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-92-162.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6161_01_000001/livy\">Link</a></td><td></td></tr><tr><td>172</td><td>application_1600172976670_6165</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6165/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-85-182.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6165_01_000001/livy\">Link</a></td><td></td></tr><tr><td>173</td><td>application_1600172976670_6166</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6166/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-75-85.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6166_01_000001/livy\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"20G\",\n",
    " \"executorMemory\": \"20G\",\n",
    " \"executorCores\": 16,\n",
    " \"numExecutors\": 20,\n",
    " \"name\": \"weighted_trend_eval_metric\",\n",
    " \"conf\": {\"spark.dynamicAllocation.enabled\":\"false\",\n",
    "          \"spark.pyspark.python\": \"python3\",\n",
    "          \"spark.sql.broadcastTimeout\": 10000}\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T20:14:17.706548Z",
     "start_time": "2021-02-09T20:13:42.897784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>174</td><td>application_1600172976670_6167</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6167/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-81-216.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6167_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f26d915a850>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T20:14:17.917748Z",
     "start_time": "2021-02-09T20:14:17.710455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '20G', 'executorMemory': '20G', 'executorCores': 16, 'numExecutors': 20, 'name': 'amir_weighted_trend_eval', 'conf': {'spark.dynamicAllocation.enabled': 'false', 'spark.pyspark.python': 'python3', 'spark.sql.broadcastTimeout': 10000}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>164</td><td>application_1600172976670_6157</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6157/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-86-133.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6157_01_000001/livy\">Link</a></td><td></td></tr><tr><td>167</td><td>application_1600172976670_6160</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6160/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-82-135.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6160_01_000001/livy\">Link</a></td><td></td></tr><tr><td>168</td><td>application_1600172976670_6161</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6161/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-92-162.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6161_01_000001/livy\">Link</a></td><td></td></tr><tr><td>172</td><td>application_1600172976670_6165</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6165/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-85-182.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6165_01_000001/livy\">Link</a></td><td></td></tr><tr><td>173</td><td>application_1600172976670_6166</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6166/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-75-85.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6166_01_000001/livy\">Link</a></td><td></td></tr><tr><td>174</td><td>application_1600172976670_6167</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-66-222.ap-south-1.compute.internal:20888/proxy/application_1600172976670_6167/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-81-216.ap-south-1.compute.internal:8042/node/containerlogs/container_1600172976670_6167_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T20:34:44.082658Z",
     "start_time": "2021-02-09T20:34:44.016504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_date = None #keep this null if you want to use the current date\n",
    "top_k = 1 \n",
    "run_days = 7 #number of days to run and consider in the average, should be bigger or equal to 2\n",
    "\n",
    "#data location\n",
    "test_env = \"prod\" #where to read the sales i.e. l1_banner_events_base\n",
    "pred_env = \"prod\" #where to read the model file\n",
    "\n",
    "#model info if there are changes\n",
    "model = \"weighted_trendfiltering_banner_events\"\n",
    "category_Score_column_name = \"category_trend_scores\"\n",
    "\n",
    "#uncomment the print lines to debug the issues that might occure in the calculations of this measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T20:34:46.782732Z",
     "start_time": "2021-02-09T20:34:46.716684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sparkContext.addPyFile(\"/home/hadoop/analysis/common/fileutils.py\")\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType, LongType, IntegerType, DoubleType, StructType, StructField\n",
    "from datetime import datetime, timedelta, date\n",
    "from fileutils import TeleScopeFileAccessUtil as fileutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T20:34:47.207026Z",
     "start_time": "2021-02-09T20:34:47.134448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_paths(model_date_out, previous_day):\n",
    "    if model_date_out is None:\n",
    "        model_date_in = (date.today() - timedelta(days=previous_day+1)).strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        model_date_in = (datetime.strptime(model_date_out, '%Y-%m-%d') - timedelta(days= previous_day)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    start_date = (datetime.strptime(model_date_in, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    end_date = start_date\n",
    "    \n",
    "    print(f\"model date: {model_date_in}, sales dates: {start_date}, {end_date}\")\n",
    "\n",
    "    model_predictions_path = fileutil.get_path(\"model\", \"model_path\", pred_env) + f\"{model}/user_categories/model_date={model_date_in}\"\n",
    "    ground_truth_path = fileutil.get_path(\"shinra\", \"shinra_base\", test_env) + \"telescope/base/l1_banner_events_base_table\"\n",
    "    \n",
    "    return model_predictions_path, ground_truth_path, start_date, end_date, model_date_in\n",
    "\n",
    "def model_prediction_input_checking(model_predictions, category_Score_column_name):\n",
    "    if 'model_date' in model_predictions.columns:\n",
    "        raise ValueError(\"model_date in the model prediction path, specify the model_date\")\n",
    "\n",
    "    model_predictions_schema = (model_predictions\n",
    "                                .select(\"customer_id\", category_Score_column_name)\n",
    "                                .schema)\n",
    "\n",
    "    check_schema = (StructType([StructField('customer_id', LongType(), True),\n",
    "                                StructField(category_Score_column_name,\n",
    "                                            ArrayType(StructType([StructField('category_id', StringType(), True),\n",
    "                                                                  StructField('probability', DoubleType(), True)]),\n",
    "                                                      True), True)]))\n",
    "\n",
    "    if model_predictions_schema != check_schema:\n",
    "        raise ValueError(\"\"\"The prediction data customer_id should be LongType, \n",
    "            and {category_Score_column_name} should be a array which contains \n",
    "            cateogry_id(StringType) and probability(DoubleType).\n",
    "            Check prediction data schema type.\"\"\".format(category_Score_column_name=category_Score_column_name))\n",
    "        \n",
    "def ground_truth_input_checking(ground_truth):\n",
    "    ground_truth_schema = ground_truth.select(\"sales\").schema\n",
    "\n",
    "    check_schema = StructType([StructField('sales', LongType(), True)])\n",
    "\n",
    "    if ground_truth_schema != check_schema:\n",
    "        raise ValueError('The test data sales should be LongType. Check ground truth data schema type.')\n",
    "\n",
    "def order_top_k(category):\n",
    "    sorted_category = sorted(category, key=lambda x: x[1], reverse=True)\n",
    "    top_k_category = [i[0] for i in sorted_category[:top_k]]\n",
    "    return top_k_category\n",
    "\n",
    "def get_top_user_categories(model_predictions_user_category):\n",
    "    top_predictions_per_user = (model_predictions_user_category\n",
    "                                .select('customer_id',\n",
    "                                        F.udf(order_top_k, ArrayType(StringType()))(F.col('category')).alias(\n",
    "                                            'pred_topk_cma_id')))\n",
    "    return top_predictions_per_user\n",
    "\n",
    "def get_user_sales(ground_truth, start_date, end_date):\n",
    "    sales_categories = (ground_truth\n",
    "                        .filter(F.col(\"dt\").between(start_date, end_date))\n",
    "                        .filter((F.col('sales').isNotNull()) & (F.col('sales') > 0))\n",
    "                        .select(F.col(\"customer_id\").cast(LongType()), \"cma_id\"))\n",
    "    return sales_categories\n",
    "\n",
    "def agg_sales_categories(sales_categories):\n",
    "    agg_categories_per_user = (sales_categories\n",
    "                               .groupBy('customer_id')\n",
    "                               .agg(F.collect_set(\"cma_id\").alias('cma_id'))\n",
    "                               .select('customer_id',\n",
    "                                       'cma_id',\n",
    "                                       F.size(F.col(\"cma_id\")).alias(\"relevant_cat_cnt\")))\n",
    "    return agg_categories_per_user\n",
    "\n",
    "def count_pred_lab_k(predictions, labels, top_k):\n",
    "    return len(set(labels).intersection(set(predictions[:top_k])))\n",
    "\n",
    "def precision_at_each_user_k(relevant_documents, ks):\n",
    "    precision_per_user_k = (relevant_documents\n",
    "                            .crossJoin(ks)\n",
    "                            .withColumn('pred_topk_cma_id', F.when(F.col('pred_topk_cma_id').isNull(), F.array([]))\n",
    "                                        .otherwise(F.col('pred_topk_cma_id')))\n",
    "                            .withColumn('cnt_pred_lab_k',\n",
    "                                        F.udf(count_pred_lab_k)(F.col('pred_topk_cma_id'), F.col('cma_id'), F.col('k')))\n",
    "                            .withColumn('perc_pred_lab_k', F.col('cnt_pred_lab_k') / F.col('k'))\n",
    "                            .withColumn('recall_pred_lab_k', F.when(F.col(\"relevant_cat_cnt\") > 0, F.col('cnt_pred_lab_k') / F.col('relevant_cat_cnt'))\n",
    "                                        .otherwise(1)))\n",
    "    return precision_per_user_k\n",
    "\n",
    "def agg_precision_at_each_k(precision_per_user_k):  # fn name\n",
    "    precision_per_k = (precision_per_user_k\n",
    "                       .withColumn('num_label', F.size('cma_id'))\n",
    "                       .withColumn('num_predictions', F.size('pred_topk_cma_id'))\n",
    "                       .withColumn('num_distinct_predictions', F.size(F.array_distinct('pred_topk_cma_id')))\n",
    "                       .withColumn('dup_pred_category', F.col('num_distinct_predictions') - F.col('num_predictions'))\n",
    "                       .groupBy(F.col('k'))\n",
    "                       .agg(F.mean('perc_pred_lab_k').alias('precision_at_k'),\n",
    "                            F.mean('recall_pred_lab_k').alias(\"recall_at_k\"),\n",
    "                            F.sum('dup_pred_category').alias('dup_pred_category_at_k'),\n",
    "                            F.sum('num_label').alias('num_label_at_k'))\n",
    "                       .select('k', 'precision_at_k', 'recall_at_k', 'dup_pred_category_at_k', 'num_label_at_k').orderBy(F.col('k')))\n",
    "    return precision_per_k\n",
    "\n",
    "def prediction_test_error_checking(precision_per_k):\n",
    "    error_checking = (precision_per_k\n",
    "                      .agg(F.sum('dup_pred_category_at_k').alias('tlt_dup_pred_category'),\n",
    "                           F.sum('num_label_at_k').alias('tlt_num_label'))\n",
    "                      .select('tlt_dup_pred_category', 'tlt_num_label'))\n",
    "    try:\n",
    "        if (error_checking.collect()==None):\n",
    "            raise ValueError(\"Empty ground truth set, check input data\")\n",
    "        if int(error_checking.collect()[0]['tlt_dup_pred_category']) > 0:\n",
    "            raise ValueError(\"Duplicates in predictions, check prediction data\")\n",
    "        if int(error_checking.collect()[0]['tlt_num_label']) == 0:\n",
    "            raise ValueError(\"Empty ground truth set, check input data\")\n",
    "    except:\n",
    "        print(\"prediction test failed.\")\n",
    "        \n",
    "def precision_recall_at_k_cal(relevant_documents, ks):\n",
    "    # This the main function that contains every function about precision@k\n",
    "\n",
    "    precision_per_user_k = precision_at_each_user_k(relevant_documents, ks)\n",
    "\n",
    "    precision_per_k = agg_precision_at_each_k(precision_per_user_k).cache()\n",
    "\n",
    "    prediction_test_error_checking(precision_per_k)\n",
    "\n",
    "    precision_recall_at_k = precision_per_k.select('k', 'precision_at_k', 'recall_at_k')\n",
    "\n",
    "    return precision_recall_at_k\n",
    "\n",
    "def get_model_categories(df):\n",
    "    return df.select(df.customer_id, F.explode(df.category).alias(\"category_scores_exp\")) \\\n",
    "        .withColumn(\"cma_id\", F.col(\"category_scores_exp\").getItem(\"category_id\")) \\\n",
    "        .select(F.col(\"cma_id\").cast(\"long\")).distinct()\n",
    "\n",
    "def get_customers_with_sales_in_model_cats(model_categories, sales_categories):\n",
    "    customers_with_sales_in_non_model_cats = (sales_categories.join(F.broadcast(model_categories\n",
    "                                                                                .withColumn(\"isModelCat\", F.lit(1))),\n",
    "                                                                    on=[\"cma_id\"], how=\"left\")\n",
    "                                              .where(F.col(\"isModelCat\").isNull()).select(\"customer_id\").distinct())\n",
    "\n",
    "    customers_with_sales_in_model_cats = (sales_categories.join(F.broadcast(model_categories),\n",
    "                                                                on=[\"cma_id\"], how=\"inner\")\n",
    "                                          .select(\"customer_id\").distinct().cache())\n",
    "\n",
    "    num_customers_to_remove = (customers_with_sales_in_non_model_cats.join(customers_with_sales_in_model_cats\n",
    "                                                                           .withColumn(\"isModelCat\", F.lit(1)),\n",
    "                                                                           on=[\"customer_id\"], how=\"full\")\n",
    "                               .where(F.col(\"isModelCat\").isNull()).count())\n",
    "\n",
    "    return num_customers_to_remove, customers_with_sales_in_model_cats\n",
    "\n",
    "\n",
    "def get_ranking_metrics_at_k(top_predictions_per_user, sales_categories, \\\n",
    "                             model_predictions_user_category, top_k, model_date):\n",
    "    agg_categories_per_user = agg_sales_categories(sales_categories)\n",
    "\n",
    "#     print(\"number of customers in prediction dataset: {count}\".format(count=top_predictions_per_user.count()))\n",
    "#     print(\"number of test customers in total: {count}\".format(count=agg_categories_per_user.count()))\n",
    "\n",
    "    # filter for customers that we could predict on\n",
    "    model_categories = get_model_categories(model_predictions_user_category).cache()\n",
    "    num_customers_to_remove, relevant_customers = get_customers_with_sales_in_model_cats(model_categories,\n",
    "                                                                                         sales_categories)\n",
    "\n",
    "#     print(\n",
    "#         f\"number of test customers that are removed due to not having sales in model categories: {num_customers_to_remove}\")\n",
    "\n",
    "    agg_categories_per_user = agg_categories_per_user.join(relevant_customers, on=[\"customer_id\"], how=\"inner\")\n",
    "\n",
    "    relevant_documents = (agg_categories_per_user\n",
    "                          .join(top_predictions_per_user, on='customer_id', how='left')\n",
    "                          .select('customer_id', 'pred_topk_cma_id', 'cma_id', 'relevant_cat_cnt'))\n",
    "\n",
    "#     print(\"relevant documents: {count}\".format(count=relevant_documents.count()))\n",
    "#     print('number of test customers are been predicted: {count}'.format(count=relevant_documents\n",
    "#                                                                         .filter(F.col('cma_id').isNotNull()).count()))\n",
    "#     print('number of test customers are not been predicted: {count}'.format(count=relevant_documents\n",
    "#                                                                             .filter(F.col('cma_id').isNull()).count()))\n",
    "\n",
    "    ks = spark.createDataFrame(list(range(1, top_k + 1)), IntegerType()).toDF(\"k\")\n",
    "\n",
    "    ks = ks.withColumn(\"dt\", F.lit(model_date))\n",
    "\n",
    "    precision_recall_at_k = precision_recall_at_k_cal(relevant_documents, ks)\n",
    "    return precision_recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T20:34:48.634252Z",
     "start_time": "2021-02-09T20:34:48.552456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main(model_predictions_path, ground_truth_path, start_date, end_date, model_date):\n",
    "    model_predictions = (spark.read.parquet(model_predictions_path))\n",
    "    ground_truth = (spark.read.parquet(ground_truth_path))\n",
    "\n",
    "    model_prediction_input_checking(model_predictions, category_Score_column_name)\n",
    "    ground_truth_input_checking(ground_truth)\n",
    "\n",
    "    model_predictions_user_category = (model_predictions\n",
    "                                       .select(\"customer_id\", F.col(category_Score_column_name).alias(\"category\")))\n",
    "\n",
    "    top_predictions_per_user = get_top_user_categories(model_predictions_user_category)\n",
    "\n",
    "    sales_categories = get_user_sales(ground_truth, start_date, end_date)\n",
    "    \n",
    "    #print(\"count of categories in the model: {count}\".format(count=get_model_categories(model_predictions_user_category).count()))\n",
    "    \n",
    "    precision_at_k = get_ranking_metrics_at_k(top_predictions_per_user, sales_categories, model_predictions_user_category,\n",
    "                                              top_k, model_date).withColumn(\"dt\", F.lit(model_date)).cache()\n",
    "    \n",
    "    precision_at_k.show(200, False)\n",
    "    \n",
    "    return precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T20:42:35.792835Z",
     "start_time": "2021-02-09T20:34:51.902208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model date: 2021-02-07, sales dates: 2021-02-08, 2021-02-08\n",
      "+---+------------------+-------------------+----------+\n",
      "|k  |precision_at_k    |recall_at_k        |dt        |\n",
      "+---+------------------+-------------------+----------+\n",
      "|1  |0.3860008220080404|0.37076421954573063|2021-02-07|\n",
      "+---+------------------+-------------------+----------+"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "model_predictions_path, ground_truth_path, start_date, end_date, model_date_update = get_paths(model_date, i)\n",
    "precision_at_k = main(model_predictions_path, ground_truth_path, start_date, end_date, model_date_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T21:40:35.290766Z",
     "start_time": "2021-02-09T20:42:35.795483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rolling day num 2: \n",
      "model date: 2021-02-06, sales dates: 2021-02-07, 2021-02-07\n",
      "+---+------------------+------------------+----------+\n",
      "|k  |precision_at_k    |recall_at_k       |dt        |\n",
      "+---+------------------+------------------+----------+\n",
      "|1  |0.5190324220081259|0.3135256228803216|2021-02-06|\n",
      "+---+------------------+------------------+----------+\n",
      "\n",
      "rolling day num 3: \n",
      "model date: 2021-02-05, sales dates: 2021-02-06, 2021-02-06\n",
      "+---+------------------+------------------+----------+\n",
      "|k  |precision_at_k    |recall_at_k       |dt        |\n",
      "+---+------------------+------------------+----------+\n",
      "|1  |0.5192057449699166|0.3176307077425393|2021-02-05|\n",
      "+---+------------------+------------------+----------+\n",
      "\n",
      "rolling day num 4: \n",
      "model date: 2021-02-04, sales dates: 2021-02-05, 2021-02-05\n",
      "+---+----------------+------------------+----------+\n",
      "|k  |precision_at_k  |recall_at_k       |dt        |\n",
      "+---+----------------+------------------+----------+\n",
      "|1  |0.52313078240778|0.3194332033970082|2021-02-04|\n",
      "+---+----------------+------------------+----------+\n",
      "\n",
      "rolling day num 5: \n",
      "model date: 2021-02-03, sales dates: 2021-02-04, 2021-02-04\n",
      "+---+------------------+-------------------+----------+\n",
      "|k  |precision_at_k    |recall_at_k        |dt        |\n",
      "+---+------------------+-------------------+----------+\n",
      "|1  |0.5624042437363542|0.34727310592133126|2021-02-03|\n",
      "+---+------------------+-------------------+----------+\n",
      "\n",
      "rolling day num 6: \n",
      "model date: 2021-02-02, sales dates: 2021-02-03, 2021-02-03\n",
      "+---+------------------+------------------+----------+\n",
      "|k  |precision_at_k    |recall_at_k       |dt        |\n",
      "+---+------------------+------------------+----------+\n",
      "|1  |0.5710495843331385|0.3682475971982918|2021-02-02|\n",
      "+---+------------------+------------------+----------+\n",
      "\n",
      "rolling day num 7: \n",
      "model date: 2021-02-01, sales dates: 2021-02-02, 2021-02-02\n",
      "+---+------------------+-----------------+----------+\n",
      "|k  |precision_at_k    |recall_at_k      |dt        |\n",
      "+---+------------------+-----------------+----------+\n",
      "|1  |0.5937011279862764|0.412070734233687|2021-02-01|\n",
      "+---+------------------+-----------------+----------+"
     ]
    }
   ],
   "source": [
    "for i in range(2,run_days+1):\n",
    "    print(f\"rolling day num {i}: \")\n",
    "    model_predictions_path, ground_truth_path, start_date, end_date, model_date_update = get_paths(model_date, i)\n",
    "    precision_at_k = precision_at_k.union(main(model_predictions_path, ground_truth_path, start_date, end_date, model_date_update))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T21:40:36.670964Z",
     "start_time": "2021-02-09T21:40:35.293476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-------------------+----------+\n",
      "|k  |precision_at_k    |recall_at_k        |dt        |\n",
      "+---+------------------+-------------------+----------+\n",
      "|1  |0.3860008220080404|0.37076421954573063|2021-02-07|\n",
      "|1  |0.5190324220081259|0.3135256228803216 |2021-02-06|\n",
      "|1  |0.5192057449699166|0.3176307077425393 |2021-02-05|\n",
      "|1  |0.52313078240778  |0.3194332033970082 |2021-02-04|\n",
      "|1  |0.5624042437363542|0.34727310592133126|2021-02-03|\n",
      "|1  |0.5710495843331385|0.3682475971982918 |2021-02-02|\n",
      "|1  |0.5937011279862764|0.412070734233687  |2021-02-01|\n",
      "+---+------------------+-------------------+----------+"
     ]
    }
   ],
   "source": [
    "precision_at_k.show(200, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-09T21:40:39.243349Z",
     "start_time": "2021-02-09T21:40:36.673303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall Averages:\n",
      "+---+-----------------+-------------------+\n",
      "|k  |precision_at_k   |recall_at_k        |\n",
      "+---+-----------------+-------------------+\n",
      "|1  |0.524932103921376|0.34984931298841576|\n",
      "+---+-----------------+-------------------+"
     ]
    }
   ],
   "source": [
    "print(\"overall Averages:\")\n",
    "precision_at_k.groupBy(\"k\").agg(F.avg(\"precision_at_k\").alias(\"precision_at_k\"), \n",
    "                                F.avg(\"recall_at_k\").alias(\"recall_at_k\")).show(200, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
