{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T15:27:07.781999Z",
     "start_time": "2020-08-25T15:27:01.113182Z"
    },
    "papermill": {
     "duration": 95.987285,
     "end_time": "2019-09-18T19:34:56.363686",
     "exception": false,
     "start_time": "2019-09-18T19:33:20.376401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master = \"local[*]\"\n",
    "launcher.conf.spark.driver.memory= \"30G\"\n",
    "launcher.conf.set(\"spark.hadoop.fs.s3a.connection.maximum\", \"8192\")\n",
    "launcher.conf.set(\"spark.hadoop.fs.s3a.threads.max\", \"2048\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T15:27:12.266298Z",
     "start_time": "2020-08-25T15:27:07.783840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4042\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1598369227341)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import java.time.LocalDate\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.{SparkSession, Row, DataFrame, Dataset}\n",
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.expressions.UserDefinedFunction\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.types._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.time.LocalDate\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.{SparkSession, Row, DataFrame, Dataset}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.expressions.UserDefinedFunction\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T22:08:33.112944Z",
     "start_time": "2020-08-25T22:08:33.016144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: String = ABTesting\n",
       "env: String = stg\n",
       "date: String = 2020-08-21\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = \"ABTesting\"  //\"noisy_clicks\"\n",
    "val env = \"stg\"\n",
    "val date = \"2020-08-21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T22:08:33.248521Z",
     "start_time": "2020-08-25T22:08:33.114075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelPredictionsPath: String = s3a://midgar-aws-workspace/stg/shinra/modelling/ABTesting/weighted_trendfiltering_banner_events/user_categories/model_date=2020-08-21\n",
       "categoryScoreColumnName: String = category_trend_scores\n",
       "groundTruthPath: String = s3a://midgar-aws-workspace/prod/shinra/telescope/base/l1_banner_events_base_table\n",
       "topK: Int = 4\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var modelPredictionsPath = s\"s3a://midgar-aws-workspace/${env}/shinra/modelling/$model/weighted_trendfiltering_banner_events/user_categories/model_date=$date\"\n",
    "val categoryScoreColumnName = \"category_trend_scores\"\n",
    "val groundTruthPath = s\"s3a://midgar-aws-workspace/prod/shinra/telescope/base/l1_banner_events_base_table\"\n",
    "val topK = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T22:08:38.742762Z",
     "start_time": "2020-08-25T22:08:36.310336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|size(category_trend_scores)|\n",
      "+---------------------------+\n",
      "|64                         |\n",
      "+---------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(modelPredictionsPath).select(size($\"category_trend_scores\")).show(1, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T15:28:13.264652Z",
     "start_time": "2020-08-25T15:28:12.187526Z"
    },
    "papermill": {
     "duration": 2.50338,
     "end_time": "2019-09-18T19:35:05.708921",
     "exception": false,
     "start_time": "2019-09-18T19:35:03.205541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.{SparkSession, Row, DataFrame, Dataset}\n",
       "defined class CustomerAffinities\n",
       "getTopUserCategories: (userAffinities: org.apache.spark.sql.Dataset[CustomerAffinities], topK: Int, filterIds: Seq[String])org.apache.spark.rdd.RDD[(Long, Array[String])]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.{SparkSession, Row, DataFrame, Dataset}\n",
    "\n",
    "case class CustomerAffinities(customer_id: Long, category_scores: Seq[(String, Double)])\n",
    "def getTopUserCategories(\n",
    "  userAffinities: Dataset[CustomerAffinities],\n",
    "  topK: Int,\n",
    "  filterIds:Seq[String]\n",
    "): RDD[(Long, Array[String])] = {\n",
    "  import userAffinities.sparkSession.implicits._\n",
    "  userAffinities.map {\n",
    "    case CustomerAffinities(customerId: Long, probArray: Seq[(String, Double)]) =>\n",
    "      (customerId, probArray.toArray.sortBy(-_._2).take(topK).map(_._1))\n",
    "  }.rdd\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T15:28:13.514725Z",
     "start_time": "2020-08-25T15:28:13.267806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getUserSales: (spark: org.apache.spark.sql.SparkSession, startDateStr: String, endDateStr: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getUserSales(spark:SparkSession, startDateStr: String,endDateStr: String): DataFrame={\n",
    "    spark.read.parquet(groundTruthPath)\n",
    "    .filter($\"dt\".between(startDateStr,endDateStr))\n",
    "    .filter($\"sales\".isNotNull &&  $\"sales\" > 0)\n",
    "    .select($\"customer_id\".cast(LongType), $\"cma_id\", $\"sales\", $\"dt\".cast(StringType))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T15:32:53.096053Z",
     "start_time": "2020-08-25T15:32:52.797964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getTopSalesCategories: (sales: org.apache.spark.sql.DataFrame)org.apache.spark.rdd.RDD[(Long, Array[String])]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "def getTopSalesCategories(sales: DataFrame): RDD[(Long, Array[String])] = {\n",
    "  import org.apache.spark.mllib.rdd.MLPairRDDFunctions.fromPairRDD\n",
    "  import sales.sparkSession.implicits._\n",
    "    \n",
    "  sales.rdd.\n",
    "    map {\n",
    "      case Row(customerId: Long, cma_id: String, sales: Long, dt: String) =>\n",
    "        (customerId, (cma_id, sales, dt))\n",
    "    }\n",
    "    .topByKey(1)(Ordering.by[(String, Long, String), (Long, String)](a =>\n",
    "      (a._2, a._3))(Ordering.fromLessThan[(Long, String)]((x, y) =>\n",
    "      if (y._1 != x._1) x._1 < y._1 else y._2 < x._2))).\n",
    "    map {\n",
    "      case (customerId: Long, recentCategories: Array[(String, Long, String)]) =>\n",
    "        (customerId, recentCategories.map(_._1))\n",
    "    }\n",
    "}*/\n",
    "\n",
    "\n",
    "def getTopSalesCategories(sales: DataFrame): RDD[(Long, Array[String])] = {\n",
    "  import org.apache.spark.mllib.rdd.MLPairRDDFunctions.fromPairRDD\n",
    "  import sales.sparkSession.implicits._\n",
    "   \n",
    "    \n",
    "    val modelCategories = Seq(\"9\", \"10\", \"12\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\",\n",
    "    \"20\", \"22\", \"23\", \"24\", \"25\", \"26\", \"29\",\n",
    "    \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\",\n",
    "    \"40\", \"41\", \"42\", \"43\", \"44\", \"45\", \"46\", \"48\", \"49\",\n",
    "    \"50\", \"51\", \"52\", \"53\", \"55\", \"56\", \"57\", \"58\", \"59\",\n",
    "    \"60\", \"352\", \"391\", \"357\", \"358\",\n",
    "    \"420\", \"421\", \"423\", \"426\", \"429\", \"437\", \"442\", \"443\", \"444\", \"445\", \"446\",\n",
    "    \"6803\", \"6797\", \"6808\", \"6809\", \"6810\", \"6811\")\n",
    "    \n",
    "  val nsales = sales.filter($\"cma_id\".isin(modelCategories:_*))  \n",
    "    \n",
    "  nsales.rdd.\n",
    "    map {\n",
    "      case Row(customerId: Long, cma_id: String, sales: Long, dt: String) =>\n",
    "        (customerId, (cma_id, sales, dt))\n",
    "    }\n",
    "    .groupByKey()\n",
    "    .map {\n",
    "      case (customerId: Long, recentCategories: Iterable[(String, Long, String)]) =>\n",
    "        (customerId, recentCategories.toArray.map(_._1))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T21:32:56.335469Z",
     "start_time": "2020-08-25T21:32:56.124340Z"
    },
    "papermill": {
     "duration": 0.45992,
     "end_time": "2019-09-18T19:35:06.225035",
     "exception": false,
     "start_time": "2019-09-18T19:35:05.765115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.reflect.ClassTag\n",
       "import org.apache.spark.mllib.evaluation.{MultilabelMetrics, RankingMetrics}\n",
       "import org.apache.spark.rdd.RDD\n",
       "defined class FixedRankingMetrics\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.reflect.ClassTag\n",
    "import org.apache.spark.mllib.evaluation.{MultilabelMetrics, RankingMetrics}\n",
    "import org.apache.spark.rdd.RDD\n",
    "class FixedRankingMetrics[T: ClassTag](predictionAndLabels: RDD[(Array[T], Array[T])]) extends RankingMetrics(predictionAndLabels: RDD[(Array[T], Array[T])]) {\n",
    "\n",
    "  override def precisionAt(k: Int): Double = {\n",
    "    require(k > 0, \"ranking position k should be positive\")\n",
    "    predictionAndLabels.map {\n",
    "      case (pred, lab) =>\n",
    "        val predSetSize = pred.toSet.size\n",
    "        val labSet = lab.toSet\n",
    "        val labSetSize = labSet.size\n",
    "        if (predSetSize != pred.length) {\n",
    "          logError(\"Duplicates in predictions, check prediction data\")\n",
    "          throw new IllegalArgumentException(\"Duplicates in predictions, check prediction data\")\n",
    "        } else if (labSet.nonEmpty) {\n",
    "          val n = math.min(pred.length, k)\n",
    "          var i = 0\n",
    "          var cnt = 0\n",
    "          while (i < n) {\n",
    "            if (labSet.contains(pred(i))) {\n",
    "              cnt += 1\n",
    "            }\n",
    "            i += 1\n",
    "          }\n",
    "          cnt.toDouble / math.min(predSetSize, k)\n",
    "        } else {\n",
    "          logWarning(\"Empty ground truth set, check input data\")\n",
    "          0.0\n",
    "        }\n",
    "    }.mean()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T15:32:59.329555Z",
     "start_time": "2020-08-25T15:32:58.899462Z"
    },
    "papermill": {
     "duration": 0.549833,
     "end_time": "2019-09-18T19:35:06.831472",
     "exception": false,
     "start_time": "2019-09-18T19:35:06.281639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getRankingMetricsAtK: (spark: org.apache.spark.sql.SparkSession, userTopCategories: org.apache.spark.rdd.RDD[(Long, Array[String])], userTopSalesCategories: org.apache.spark.rdd.RDD[(Long, Array[String])], Ks: Array[Int], registerToDataDog: Boolean, modelName: String, t1: Option[Long])org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  def getRankingMetricsAtK(\n",
    "    spark: SparkSession,\n",
    "    userTopCategories: RDD[(Long, Array[String])],\n",
    "    userTopSalesCategories: RDD[(Long, Array[String])],\n",
    "    Ks: Array[Int],\n",
    "    registerToDataDog: Boolean = true,\n",
    "    modelName: String = \"not-specified\",\n",
    "    t1: Option[Long] = None\n",
    "  ): DataFrame = {\n",
    "    import spark.sqlContext.implicits._\n",
    "\n",
    "    println(s\"userTopCategories: ${userTopCategories.count}\")\n",
    "    println(s\"userTopSalesCategories: ${userTopSalesCategories.count}\")\n",
    "\n",
    "    val relevantDocuments = userTopCategories.join(userTopSalesCategories).map(_._2)\n",
    "    println(s\"relevantDocuments: ${relevantDocuments.count}\")\n",
    "\n",
    "    val metrics = new FixedRankingMetrics(relevantDocuments)\n",
    "    spark.sparkContext.parallelize(Ks.map {\n",
    "      k =>\n",
    "        val precision = metrics.precisionAt(k)\n",
    "        val ndcg = metrics.ndcgAt(k)\n",
    "        val meanAP = metrics.meanAveragePrecision\n",
    "        (k, precision, ndcg, meanAP)\n",
    "    }).toDF(\"k\", \"precision_at_k\", \"ndcg_at_k\", \"mean_avg_precison\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T22:08:47.119718Z",
     "start_time": "2020-08-25T22:08:46.436261Z"
    },
    "papermill": {
     "duration": 3.969937,
     "end_time": "2019-09-18T19:35:10.878764",
     "exception": false,
     "start_time": "2019-09-18T19:35:06.908827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelPredictions: org.apache.spark.sql.Dataset[CustomerAffinities] = [customer_id: bigint, category_scores: array<struct<category_id:string,probability:double>>]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val modelPredictions = spark.read.parquet(modelPredictionsPath)\n",
    "  .select($\"customer_id\", $\"category_trend_scores\" as \"category_scores\")\n",
    "  .as[CustomerAffinities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T22:08:47.932043Z",
     "start_time": "2020-08-25T22:08:47.718602Z"
    },
    "papermill": {
     "duration": 4.791588,
     "end_time": "2019-09-18T19:35:15.732070",
     "exception": false,
     "start_time": "2019-09-18T19:35:10.940482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topPredictionsPerUser: org.apache.spark.rdd.RDD[(Long, Array[String])] = MapPartitionsRDD[149] at rdd at <console>:27\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topPredictionsPerUser = getTopUserCategories(modelPredictions, topK, Seq())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T22:08:59.161449Z",
     "start_time": "2020-08-25T22:08:51.203520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salesCategories: org.apache.spark.sql.DataFrame = [customer_id: bigint, cma_id: string ... 2 more fields]\n",
       "res10: Long = 9247304\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val salesCategories = getUserSales(spark, \"2020-08-22\", \"2020-08-22\")\n",
    "salesCategories.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T22:09:19.219266Z",
     "start_time": "2020-08-25T22:08:59.162886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topSalesPerUser: org.apache.spark.rdd.RDD[(Long, Array[String])] = MapPartitionsRDD[167] at map at <console>:84\n",
       "res11: Long = 5447855\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topSalesPerUser = getTopSalesCategories(salesCategories)\n",
    "topSalesPerUser.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T22:57:29.847548Z",
     "start_time": "2020-08-25T22:09:19.220739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userTopCategories: 101793436\n",
      "userTopSalesCategories: 5447855\n",
      "relevantDocuments: 5397946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "precisionAtK_Sales: org.apache.spark.sql.DataFrame = [k: int, precision_at_k: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val precisionAtK_Sales = getRankingMetricsAtK(\n",
    "    spark,\n",
    "    topPredictionsPerUser,\n",
    "    topSalesPerUser,\n",
    "    (1 to topK).toArray,\n",
    "    registerToDataDog = false,\n",
    "    \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T22:57:30.006763Z",
     "start_time": "2020-08-25T22:57:29.848587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+-------------------+\n",
      "|k  |precision_at_k     |ndcg_at_k          |mean_avg_precison  |\n",
      "+---+-------------------+-------------------+-------------------+\n",
      "|1  |0.3464841997307863 |0.3464841997307861 |0.44529838203992556|\n",
      "|2  |0.46412996721345473|0.41928093589809523|0.44529838203992556|\n",
      "|3  |0.5671356289966589 |0.4731293462310769 |0.44529838203992556|\n",
      "|4  |0.6449846694773652 |0.5092145561818261 |0.44529838203992556|\n",
      "+---+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precisionAtK_Sales.show(20,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "papermill": {
   "duration": 2137.739904,
   "end_time": "2019-09-18T20:08:55.528100",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/public/notebooks/snadar/Exploration/ClickEvaluation.ipynb",
   "output_path": "/home/jovyan/public/notebooks/snadar/Exploration/WeightedTrendEvaluation.ipynb",
   "parameters": {
    "jobStartDateString": "2019-09-02",
    "modelName": "trendfiltering_WeightedTrend",
    "validationOutputName": "trendfiltering_WeightedTrend"
   },
   "start_time": "2019-09-18T19:33:17.788196",
   "version": "1.0.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
